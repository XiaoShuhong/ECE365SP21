{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 3: Language Modeling\n",
    "=============\n",
    "In this problem set, your objective is to train a language model, evaluate it and explore how it can be used for language generation. Towards that end you will:\n",
    "\n",
    "- Train an n-gram language model.\n",
    "- Use that language model to generate representative sentences.\n",
    "- Study the effect of training data size, and language model complexity (n-gram size), on the modeling capacity of a language model.\n",
    "\n",
    "- **For this assignment, submit ```lab3.py``` on Gradescope.**\n",
    "- In order to test the lab you can run ```python run_tests.py``` or ```python run_tests.py -j``` (more detailed information)\n",
    "- In order to install the correct dependencies you can run ```pip install -r requirements.txt```\n",
    "\n",
    "Total points: 90 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "In order to develop this assignment, you will need [python 3.6](https://www.python.org/downloads/) and the following libraries. Most if not all of these are part of [anaconda](https://www.continuum.io/downloads), so a good starting point would be to install that.\n",
    "\n",
    "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
    "- [nosetests](https://nose.readthedocs.org/en/latest/)\n",
    "- [nltk](https://www.nltk.org)\n",
    "\n",
    "Here is some help on installing packages in python: https://packaging.python.org/installing/. You can use ```pip --user``` to install locally without sudo. We have also provided a requirements.txt file with the correct packages and their respective versions, so you can also run ```pip install -r requirements.txt``` to install the correct dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload\n",
    "from collections import defaultdict\n",
    "import lab3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python version\n",
      "python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "print('My Python version')\n",
    "\n",
    "print('python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My library versions\n",
      "nose: 1.3.7\n",
      "nltk: 3.4.5\n"
     ]
    }
   ],
   "source": [
    "print('My library versions')\n",
    "\n",
    "print('nose: {}'.format(nose.__version__))\n",
    "print('nltk: {}'.format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your libraries are the right version, run:\n",
    "\n",
    "`nosetests tests/test_environment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.000s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training a language model\n",
    "\n",
    "Let us first train a 3-gram language model. We need a monolingual corpus, which we will get using nltk.\n",
    "\n",
    "Total: 40 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first extract from nltk's reuters corpus, 2 corpora in 2 different domains (here, subject areas), the food industry and the natural resources industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "food = ['barley', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copra-cake''grain', 'groundnut', 'groundnut-oil', 'potato''soy-meal', 'soy-oil', 'soybean', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'veg-oil', 'wheat']\n",
    "natural_resources = ['alum', 'fuel', 'gas', 'gold', 'iron-steel', 'lead', 'nat-gas', 'palladium', 'propane', 'tin', 'zinc']\n",
    "corpus = nltk.corpus.reuters\n",
    "food_corpus = corpus.raw(categories=food)\n",
    "natr_corpus = corpus.raw(categories=natural_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Your first task is to tokenize the raw text into a list of sentences, which are in turn a list of words. No need for any other kind of preprocessing such as lowercasing.\n",
    "\n",
    "- **Deliverable 1.1**: Complete the function `lab3.tokenize`. (5 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_1_tk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_corpus_tk = lab3.tokenize_corpus(food_corpus)\n",
    "natr_corpus_tk = lab3.tokenize_corpus(natr_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monday'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_corpus_tk[25][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natr_corpus_tk[25][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#food_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Your second task is to pad your sentences with the start-of-sentence symbol `'<s>'` and end-of-sentence symbol `'</s>'`. These symbols are necessary to model the probability of words that usually start a sentence and those that usually end a sentence.\n",
    "\n",
    "- **Deliverable 1.2**: Complete the function `lab3.pad`. (5 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_2_pad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=[1,2,3]\n",
    "# b=copy.copy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#food_corpus_tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_corpus_tk_pd = lab3.pad_corpus(food_corpus_tk)\n",
    "natr_corpus_tk_pd = lab3.pad_corpus(natr_corpus_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "#food_corpus_tk_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_corpus_tk_pd[35][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natr_corpus_tk_pd[35][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food_corpus_tk_pd[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#food_corpus_tk_pd[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(natr_corpus_tk_pd[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food_corpus_tk_pd[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food_corpus_tk[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Other',\n",
       " 'countries',\n",
       " 'are',\n",
       " 'already',\n",
       " 'cutting',\n",
       " 'into',\n",
       " 'the',\n",
       " 'U.S.',\n",
       " 'Market',\n",
       " 'share',\n",
       " 'here',\n",
       " '.']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_corpus_tk[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(food_corpus_tk_pd[45]) - len(food_corpus_tk[45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Your third task is to split the corpora into train, for training the language model, and test, for testing the language model. We will go with the traditional 80% (train), 20% (test) split. The first `floor(0.8*num_of_tokens)` should constitute the training corpus, and the rest should constitute the test corpus.\n",
    "\n",
    "- **Deliverable 1.3**: Complete the function `lab3.split_corpus`. (5 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_3_spc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(food_corpus_tk_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_corpus_tr, food_corpus_te = lab3.split_corpus(food_corpus_tk_pd)\n",
    "natr_corpus_tr, natr_corpus_te = lab3.split_corpus(natr_corpus_tk_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by'"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_corpus_te[3][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Project'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natr_corpus_te[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(food_corpus_te)+len(food_corpus_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into n-grams\n",
    "\n",
    "Your fourth task is to count n-grams in the text up to a specific order.\n",
    "\n",
    "- **Deliverable 1.4**: Complete the function `lab3.count_ngrams`. (20 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_4_cn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#food_vocab\n",
    "#food_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#food_corpus_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(food_ngrams)\n",
    "# len(food_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_ngrams, food_vocab = lab3.count_ngrams(food_corpus_tr, 3)\n",
    "natr_ngrams, natr_vocab = lab3.count_ngrams(natr_corpus_tr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ANALYSTS', 'ANALYSTS')"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_ngrams[('sold', 'the')]\n",
    "sorted(food_vocab)[3200], 'ANALYSTS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating n-gram probability\n",
    "\n",
    "Your last task in this part of the problem set is to estimate the n-gram probabilities p(w_i|w_{i-n+1}, w_{i-n+2}, .., w_{i-1}), with no smoothing. For the purposes of this exercise we will use the maximum likelihood estimate and perform no smoothing. \n",
    "\n",
    "- **Deliverable 1.5**: Complete the function `lab3.estimate`. (5 points)\n",
    "- **Test**: `tests/test_visible.py:test_d1_5_es`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4]\n",
    "a[len(a)-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(lab3.estimate(food_ngrams, ['palm'], ['producer', 'of']))\n",
    "print(lab3.estimate(natr_ngrams, ['basis'], ['tested', 'the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application: the speech recognition task takes human voice as its input and outputs text. If the pronunciation of two words are similar, Language Model can help decide which word to choose! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(food_ngrams[('there', 'is', 'no')])\n",
    "print(food_ngrams[('their', 'is', 'no')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the count of 'there is no' and 'their is no', which word ('there' or 'their') is more likely to be taken as the output? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Model is not only helpful in speech recogition, but text generation (*e.g.*, machine translation, summarization, image captioning), spelling correction and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a language model\n",
    "\n",
    "Now we will combine everything together and train our language model! One way to see what the language model has learned is to see the sentences it can generate.\n",
    "\n",
    "For the sake of simplicity, and for the purposes of later parts in this problem set, we use nltk's lm module to train a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Laplace\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "size_ngram = 3\n",
    "\n",
    "food_train, food_vocab = padded_everygram_pipeline(size_ngram, food_corpus_tk[:int(0.8*len(food_corpus_tk))])\n",
    "natr_train, natr_vocab = padded_everygram_pipeline(size_ngram, natr_corpus_tk[:int(0.8*len(natr_corpus_tk))])\n",
    "\n",
    "food_test = sum([['<s>'] + x + ['</s>'] for x in food_corpus_tk[int(0.8*len(food_corpus_tk)):]],[])\n",
    "natr_test = sum([['<s>'] + x + ['</s>'] for x in natr_corpus_tk[int(0.8*len(natr_corpus_tk)):]],[])\n",
    "\n",
    "food_lm = Laplace(size_ngram)\n",
    "natr_lm = Laplace(size_ngram)\n",
    "\n",
    "food_lm.fit(food_train, food_vocab)\n",
    "natr_lm.fit(natr_train, natr_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask our language model to generate a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', '<s>', 'GHANA', 'COCOA', 'PURCHASES', '1,323', 'TONNES', 'IN', '1987/88']\n",
      "['<s>', '<s>', '<s>', 'HAITI', 'ANNOUNCES', 'FIND', 'OF', 'ORE-RICH', 'GOLD', 'FIELD']\n"
     ]
    }
   ],
   "source": [
    "# This might take some time\n",
    "n_words = 10\n",
    "print(food_lm.generate(n_words, random_seed=3))  # random_seed makes the random sampling part of generation reproducible. \n",
    "print(natr_lm.generate(n_words, random_seed=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluating a language model\n",
    "\n",
    "Next, we evaluate our language models using the perplexity measure, and draw conclusions on how a change of domains (here, subject areas) can affect the performance of a language model. Perplexity measures the language model capacity at predicting sentences in a test corpus.\n",
    "\n",
    "Total: 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Deliverable 2.1**: Complete the function `lab3.get_perplexity`. (10 points)\n",
    "- **Test**: `tests/test_visible.py:test_d2_1_gp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.lm.models.Laplace at 0x1ac95e68988>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8685.066279657683\n",
      "8861.249998869525\n",
      "5613.3468679518755\n",
      "5662.4594180903705\n"
     ]
    }
   ],
   "source": [
    "# This might take some time\n",
    "print(lab3.get_perplexity(food_lm, food_test[:5000]))\n",
    "print(lab3.get_perplexity(food_lm, natr_test[:5000]))\n",
    "print(lab3.get_perplexity(natr_lm, natr_test[:5000]))\n",
    "print(lab3.get_perplexity(natr_lm, food_test[:5000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What observations can you make on the results? Is the domain shift affecting the performance of the language model? What are possible explanations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-75755ce98e18d29b",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your Observation**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data size and model complexity\n",
    "\n",
    "Let us now see how the size of the training data and the complexity of the model we choose affects the quality of our language model.\n",
    "\n",
    "Total: 40 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part we'd like to see the difference between a 2-gram model and a 3-gram model. Typically, with a larger n, the n-gram model gives us more information about the word sequence and has lower perplexity. \n",
    "\n",
    "For testing, we'll only be considering 5% instead of 20% of the test data for running time purposes. \n",
    "\n",
    "- **Deliverable 3.1**: Complete the function `lab3.vary_ngram`. (40 points)\n",
    "- **Test**: `tests/test_visible.py:test_d3_1_vary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lab3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 5595.699734426968, 3: 5624.330897353105}\n"
     ]
    }
   ],
   "source": [
    "n_gram_orders = [2, 3]\n",
    "\n",
    "train_corpus = natr_corpus_tk[:int(0.8*len(natr_corpus_tk))]\n",
    "test_corpus = natr_corpus_tk[int(0.8*len(natr_corpus_tk)): int(0.85*len(natr_corpus_tk))]\n",
    "\n",
    "results = lab3.vary_ngram(train_corpus, test_corpus, n_gram_orders)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we notice that the 3-gram language model actually performs worse than the 2-gram language model. This is due to the small size of the training corpus. A 3-gram language model is actually too complex of a model for a small training size. If our training data was larger, we would be seeing the opposite. If we trained 1-gram, 2-gram, and 3-gram models on 38 million words from the Wall Street Journal, we will get perplexity of 962, 170, 109 respectively on a test set of 1.5 million words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see a few examples of top frequent n-gram examples. Let's start with unigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',',), ('the',), ('<s>',), ('</s>',), ('.',), ('of',), ('to',), ('and',), ('said',), ('in',), ('a',), ('for',), ('The',), ('from',), ('pct',), ('mln',), ('at',), ('on',), (\"'s\",), ('is',)]\n"
     ]
    }
   ],
   "source": [
    "natr_ngrams, natr_vocab = lab3.count_ngrams(natr_corpus_tr, 3)\n",
    "\n",
    "top_ngram = []\n",
    "count = 0\n",
    "for i in sorted(natr_ngrams.items(), key=lambda x: x[1], reverse=True):\n",
    "    if len(i[0]) == 1:\n",
    "        top_ngram.append(i[0])\n",
    "        count += 1\n",
    "    if count >=20:\n",
    "        break\n",
    "print(top_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think unigram captures any grammatical information? How well do you think unigram captures the language information? \n",
    "\n",
    "Now let's see bigram and trigram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', '</s>'), ('said', '.'), ('<s>', 'The'), ('in', 'the'), ('of', 'the'), ('&', 'lt'), ('lt', ';'), (',', 'the'), ('said', 'it'), ('said', 'the'), ('<s>', '``'), (',', \"''\"), (',', 'which'), ('to', 'the'), ('for', 'the'), (',', 'a'), ('on', 'the'), (',', 'and'), ('mln', 'dlrs'), ('<s>', 'It')]\n",
      "[('said', '.', '</s>'), ('&', 'lt', ';'), ('.', \"''\", '</s>'), ('<s>', 'The', 'company'), ('<s>', 'It', 'said'), ('he', 'said', '.'), ('ounces', 'of', 'gold'), ('year', '.', '</s>'), ('The', 'company', 'said'), ('...', '...', '...'), ('added', '.', '</s>'), ('oil', 'and', 'gas'), (',', 'it', 'said'), ('pct', '.', '</s>'), (',', \"''\", 'he'), (',', 'he', 'said'), ('it', 'said', '.'), ('sources', 'said', '.'), ('is', 'expected', 'to'), ('<s>', 'He', 'said')]\n"
     ]
    }
   ],
   "source": [
    "top_ngram = []\n",
    "count = 0\n",
    "for i in sorted(natr_ngrams.items(), key=lambda x: x[1], reverse=True):\n",
    "    if len(i[0]) == 2:\n",
    "        top_ngram.append(i[0])\n",
    "        count += 1\n",
    "    if count >=20:\n",
    "        break\n",
    "print(top_ngram)\n",
    "\n",
    "top_ngram = []\n",
    "count = 0\n",
    "for i in sorted(natr_ngrams.items(), key=lambda x: x[1], reverse=True):\n",
    "    if len(i[0]) == 3:\n",
    "        top_ngram.append(i[0])\n",
    "        count += 1\n",
    "    if count >=20:\n",
    "        break\n",
    "print(top_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with unigram, bigram and trigram can capture more information. \n",
    "Bigram language model can already capture some of the grammatical information, such as 'in the', 'of the'. However, the power of bigram is still limited. \n",
    "The trigram can output more adequate short phrases such as 'ounces of gold', 'The company said', 'oil and gas'. \n",
    "\n",
    "Therefore, typically the n-gram model with a larger n contains more information about the word sequence and thus, has lower perplexity. However, the tradeoff is the computational efficiency and memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
